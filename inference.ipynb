{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop-aipnlp/miniforge3/envs/mdy/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "3.4.0\n",
      "2.8.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import random\n",
    "import os\n",
    "os.environ['TRITON_PRINT_AUTOTUNING'] = '1'\n",
    "import flash_attn\n",
    "print(torch.__version__)\n",
    "print(triton.__version__)\n",
    "print(flash_attn.__version__)\n",
    "def compare(x, y, prefix=\"\"):\n",
    "    if x is None or y is None:\n",
    "        return\n",
    "    assert x.shape == y.shape\n",
    "    if 0 in list(x.shape):\n",
    "        return\n",
    "    \n",
    "    if any([x.dtype == torch.float32, y.dtype==torch.float32]):\n",
    "        x,y = x.float(), y.float()\n",
    "    diff = (x-y).abs()\n",
    "    if prefix:\n",
    "        print(prefix, end=\": \")\n",
    "    print(f\"max_diff: {diff.max().item()}, mean_diff: {diff.mean().item()}\")\n",
    "\n",
    "def generate_cu_seqlens(end=8192, mean=2048, std=512):\n",
    "    r = [0]\n",
    "    while r[-1] < end:\n",
    "        a = random.randint(mean-std, mean+std)\n",
    "        r.append(r[-1] + a)\n",
    "    r[-1] = end\n",
    "    cu_seqlens = torch.tensor(r, device=torch.cuda.current_device(), dtype=torch.int32)\n",
    "    return cu_seqlens\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_tabels(x, y, cu_seqlens, seed=42, return_cmp_slot_mapping=False):\n",
    "    B = len(cu_seqlens) - 1\n",
    "    total_num_blocks, block_size, H, D = y.shape\n",
    "    y = y.view(-1, H, D)\n",
    "    \n",
    "    unused = np.arange(0, total_num_blocks)\n",
    "    # np.random.seed(seed)\n",
    "    # np.random.shuffle(unused)\n",
    "    block_tabels = []\n",
    "    context_lens = []\n",
    "    cmp_slop_mapping = []\n",
    "    off = 0\n",
    "    for i in range(B):\n",
    "        s = (cu_seqlens[i+1] - cu_seqlens[i]).item()\n",
    "        context_lens.append(s)\n",
    "        need = triton.cdiv(s, block_size)\n",
    "        block_ids = unused[off:off+need]\n",
    "        block_tabels.append(block_ids.tolist())\n",
    "        slot = block_ids[:, None] * block_size + np.arange(0, block_size)[None, :]\n",
    "        slot = slot.reshape(-1)[:s]\n",
    "        slot = torch.tensor(slot, device=x.device)\n",
    "        y[slot] = x[cu_seqlens[i]:cu_seqlens[i+1]]\n",
    "        off += need\n",
    "        if return_cmp_slot_mapping:\n",
    "            kernel_size = NSAHelper.kernel_size\n",
    "            stride = NSAHelper.stride\n",
    "            cmp_len = max((s - kernel_size) // stride + 1, 0)\n",
    "            need = triton.cdiv(cmp_len, block_size)\n",
    "            block_ids = unused[off:off+need]\n",
    "            slot = block_ids[:, None] * block_size + np.arange(0, block_size)[None, :]\n",
    "            slot = slot.reshape(-1)[:cmp_len]\n",
    "            cmp_slop_mapping.extend(slot.tolist())\n",
    "            off += need\n",
    "    maxlen = triton.cdiv(max([len(i) for i in block_tabels]), 8) * 8\n",
    "    block_tabels = [i + [-1] * (maxlen - len(i)) for i in block_tabels]\n",
    "    block_tabels = torch.tensor(block_tabels, device=x.device, dtype=torch.int32)\n",
    "    context_lens = torch.tensor(context_lens, device=x.device, dtype=torch.int32)\n",
    "    \n",
    "    if not return_cmp_slot_mapping:\n",
    "        return block_tabels, context_lens\n",
    "    else:\n",
    "        cmp_slop_mapping = torch.tensor(cmp_slop_mapping, device=x.device, dtype=torch.int32)\n",
    "        return block_tabels, context_lens, cmp_slop_mapping\n",
    "from flash_nsa import NSAHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.001953125, mean_diff: 3.981590270996094e-05\n",
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_attn_interface import flash_attn_varlen_func, flash_attn_with_kvcache\n",
    "from flash_attn import flash_attn_with_kvcache as flash_attn_with_kvcache_v2\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 128\n",
    "block_size = 128\n",
    "\n",
    "cu_seqlens_k = generate_cu_seqlens(t, 513, 0)\n",
    "cu_seqlens_q = torch.arange(len(cu_seqlens_k), device=cu_seqlens_k.device, dtype=torch.int32)\n",
    "maxlen = (cu_seqlens_k[1:] - cu_seqlens_k[:-1]).max().item()\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[cu_seqlens_k[1:]-1]\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, cu_seqlens_k)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, cu_seqlens_k)\n",
    "window_size = (512, -1)\n",
    "out1 = flash_attn_varlen_func(total_q, k, v, cu_seqlens_k, cu_seqlens_k, maxlen, maxlen, \n",
    "                            #   window_size=window_size, \n",
    "                              causal=True)\n",
    "out2 = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, window_size=window_size)\n",
    "out3 = flash_attn_with_kvcache(total_q, k_cache, v_cache, cu_seqlens_q=cu_seqlens_k, max_seqlen_q=maxlen, cache_seqlens=context_lens, page_table=block_tables, causal=True, window_size=window_size)\n",
    "compare(out1[cu_seqlens_k[1:]-1], out2.squeeze(1))\n",
    "compare(out1, out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn_interface import flash_attn_varlen_func, flash_attn_with_kvcache\n",
    "from flash_attn import flash_attn_with_kvcache as flash_attn_with_kvcache_v2\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64 * 4\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 10\n",
    "block_size = 256\n",
    "\n",
    "B = 32\n",
    "S = 16 * 1024\n",
    "t = B * S\n",
    "cu_seqlens_k = generate_cu_seqlens(t, S, 0)\n",
    "cu_seqlens_q = torch.arange(len(cu_seqlens_k), device=cu_seqlens_k.device, dtype=torch.int32)\n",
    "maxlen = (cu_seqlens_k[1:] - cu_seqlens_k[:-1]).max().item()\n",
    "q = torch.randn(len(cu_seqlens_k) - 1, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, cu_seqlens_k)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, cu_seqlens_k)\n",
    "window_size = (512, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25801832703026856\n",
      "0.26085690104693077\n",
      "0.027003388910088688\n",
      "0.16595018988820762\n"
     ]
    }
   ],
   "source": [
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables)))\n",
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache_v2(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, block_table=block_tables)))\n",
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, window_size=window_size)))\n",
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache_v2(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, block_table=block_tables, window_size=window_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25929387067268844\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compress kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.0, mean_diff: 0.0\n",
      "0.009867040589073763\n",
      "0.010524349875664736\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import mean_pooling\n",
    "from flash_nsa.inference.compress_kv import mean_pooling_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, t//4, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.randn(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.randn(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "block_tables, context_lens, cmp_slot_mapping = get_tabels(v, v_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "ref_out = mean_pooling(v)\n",
    "\n",
    "mean_pooling_prefill(v_cache, y_cu_seqlens, NSAHelper.y_maxlen, cmp_slot_mapping, block_tables)\n",
    "out = v_cache.flatten(0, 1)[cmp_slot_mapping]\n",
    "compare(ref_out, out)\n",
    "print(triton.testing.do_bench(lambda: mean_pooling(v)))\n",
    "print(triton.testing.do_bench(lambda: mean_pooling_prefill(v_cache, y_cu_seqlens, NSAHelper.y_maxlen, cmp_slot_mapping, block_tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "max_diff: 0.0, mean_diff: 0.0\n",
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.compress_kv import mean_pooling_decode, kv_mean_pooling_decode\n",
    "from flash_nsa import mean_pooling\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 8192\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 128\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 512, 0)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "get_tabels(v, v_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "block_tables, context_lens, cmp_slop_mapping = get_tabels(k, k_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "# must be not -1 in test\n",
    "cmp_slop_mapping = cmp_slop_mapping[y_cu_seqlens[1:] - 1]\n",
    "\n",
    "ref_cmpk = mean_pooling(k)[y_cu_seqlens[1:] - 1]\n",
    "ref_cmpv = mean_pooling(v)[y_cu_seqlens[1:] - 1]\n",
    "kv_mean_pooling_decode(k_cache, v_cache, cmp_slop_mapping, block_tables, context_lens)\n",
    "cmpk = k_cache.flatten(0, 1)[cmp_slop_mapping]\n",
    "cmpv = v_cache.flatten(0, 1)[cmp_slop_mapping]\n",
    "compare(ref_cmpk, cmpk)\n",
    "compare(ref_cmpv, cmpv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "0.010033327620476484\n",
      "0.012787732492674658\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.compress_kv import mean_pooling_decode, kv_mean_pooling_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 8 * 1024, 0)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "cmp_slop_mapping = torch.randint(0, total_blocks * block_size, (B,), device=device, dtype=torch.int32)\n",
    "a = mean_pooling_decode(k_cache, cmp_slop_mapping, block_tables, context_lens)\n",
    "print(triton.testing.do_bench(lambda: mean_pooling_decode(k_cache, cmp_slop_mapping, block_tables, context_lens)))\n",
    "print(triton.testing.do_bench(lambda: kv_mean_pooling_decode(k_cache, v_cache, cmp_slop_mapping, block_tables, context_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009379466746604021\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = mean_pooling_decode(k_cache, cmp_slop_mapping, block_tables, context_lens)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cmp_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 10248\n",
    "block_size = 256\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "y1,lse1 = compress_attn(q, k, v)\n",
    "y2,lse2 = cmp_attn_prefill(q, k_cache, v_cache, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)\n",
    "compare(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.081173419952393\n",
      "8.966964461586691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(triton.testing.do_bench(lambda: cmp_attn_prefill(q, k_cache, v_cache, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)))\n",
    "print(triton.testing.do_bench(lambda: compress_attn(q, k, v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "max_diff: 0.00390625, mean_diff: 0.00018787384033203125\n",
      "max_diff: 9.5367431640625e-07, mean_diff: 2.0302832126617432e-07\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_prefill, cmp_attn_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 8192\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 2048, 512)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:]-1]\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "y1,lse1 = compress_attn(total_q, k, v)\n",
    "y1 = y1[x_cu_seqlens[1:]-1]\n",
    "lse1 = lse1[:, x_cu_seqlens[1:]-1]\n",
    "y2,lse2 = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=0)\n",
    "compare(y1, y2)\n",
    "compare(lse1, lse2.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "0.07878897878666746\n",
      "0.0084408145091438\n",
      "0.08395924202929762\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_prefill, cmp_attn_decode\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:]-1]\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "# y1,_ = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=True)\n",
    "# y2,_ = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=False)\n",
    "# compare(y1, y2)\n",
    "cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=True, bench=True)\n",
    "print(triton.testing.do_bench(lambda:cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=True), warmup=100, rep=100))\n",
    "\n",
    "# print(triton.testing.do_bench(lambda:cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=False), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08242805514048368\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4, tma=True, bench=False)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "7.129031401414138\n",
      "6.519980843861898\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices\n",
    "from flash_nsa.inference.topk import topk_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 4096\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "# x_cu_seqlens = generate_cu_seqlens(t, 32, 24)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, lse = compress_attn(q, k, v)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "topk1, _ = slc_topk_indices(q, k, lse)\n",
    "topk2 = topk_prefill(q, k_cache, lse, x_cu_seqlens, NSAHelper.x_maxlen, NSAHelper.y_maxlen, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=num_slc_blocks)\n",
    "topk1.masked_fill_(topk1==topk1.max(), -1)\n",
    "print((topk1 != topk2).sum())\n",
    "print(triton.testing.do_bench(lambda: slc_topk_indices(q, k, lse)))\n",
    "print(triton.testing.do_bench(lambda: topk_prefill(q, k_cache, lse, x_cu_seqlens, NSAHelper.x_maxlen, NSAHelper.y_maxlen, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=num_slc_blocks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices\n",
    "from flash_nsa.inference.topk import topk_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 128\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 1024, 512)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:] - 1]\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, total_lse = compress_attn(total_q, k, v)\n",
    "lse = total_lse.transpose(0,1)[x_cu_seqlens[1:] - 1]\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "topk1, _ = slc_topk_indices(total_q, k, total_lse, ignore_index=2048, maybe_efficient_version=False, fp32=True)\n",
    "topk1.masked_fill_(topk1==topk1.max(), -1)\n",
    "topk1 = topk1[:, x_cu_seqlens[1:] - 1]\n",
    "\n",
    "topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)\n",
    "print((topk1 != topk2).sum())\n",
    "# print(triton.testing.do_bench(lambda: slc_topk_indices(q, k, lse)))\n",
    "# print(triton.testing.do_bench(lambda: topk_prefill(q, k_cache, lse, x_cu_seqlens, NSAHelper.x_maxlen, NSAHelper.y_maxlen, block_tables, NSAHelper.x_seqlens, num_slc_blocks=num_slc_blocks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "0.05582140394122593\n",
      "0.04515248586357028\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.topk import topk_decode\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 8 * 1024, 512)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(len(x_cu_seqlens)-1, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "\n",
    "_, lse = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4)\n",
    "topk1 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)\n",
    "topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, persistent=True, workers=4)\n",
    "print((topk1-topk2).sum())\n",
    "print(triton.testing.do_bench(lambda: cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=2)))\n",
    "print(triton.testing.do_bench(lambda: topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03993252619965145\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=2048, fixed_y_maxlen=8192)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033605471474296\n"
     ]
    }
   ],
   "source": [
    "g2 = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g2):\n",
    "    topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=2048, persistent=True, workers=4)\n",
    "print(triton.testing.do_bench(lambda: g2.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# slc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.0, mean_diff: 0.0\n",
      "13.33336693899972\n",
      "13.311012676783971\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices, select_attn\n",
    "from flash_nsa.inference.select_attn import slc_attn_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1280\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, lse = compress_attn(q, k[:t2], v[:t2])\n",
    "\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "topk, _ = slc_topk_indices(q, k[:t2], lse)\n",
    "out1 = select_attn(q, k, v, topk)\n",
    "out2, _ = slc_attn_prefill(q, k_cache, v_cache, topk, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)\n",
    "compare(out1, out2)\n",
    "# print((topk1 != topk2).sum())\n",
    "print(triton.testing.do_bench(lambda: select_attn(q, k, v, topk)))\n",
    "print(triton.testing.do_bench(lambda: slc_attn_prefill(q, k_cache, v_cache, topk, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.001953125, mean_diff: 6.67572021484375e-05\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices, select_attn\n",
    "from flash_nsa.inference.select_attn import slc_attn_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1280\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 1024, 512)\n",
    "# x_cu_seqlens = generate_cu_seqlens(t, t, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:]-1]\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, lse = compress_attn(total_q, k[:t2], v[:t2])\n",
    "\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "total_topk, _ = slc_topk_indices(total_q, k[:t2], lse)\n",
    "topk = total_topk[:, x_cu_seqlens[1:]-1]\n",
    "# topk = topk.sort(-1)[0]\n",
    "out1 = select_attn(total_q, k, v, total_topk)\n",
    "out1 = out1[x_cu_seqlens[1:]-1]\n",
    "out2, _ = slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens)\n",
    "compare(out1, out2)\n",
    "# print((topk1 != topk2).sum())\n",
    "# print(triton.testing.do_bench(lambda: select_attn(q, k, v, topk)))\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_prefill(q, k_cache, v_cache, topk, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.00048828125, mean_diff: 2.9831426218152046e-09\n",
      "max_diff: 0.001953125, mean_diff: 9.918212890625e-05\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.select_attn import slc_attn_decode, fused_slc_swa_attn_decode\n",
    "from flash_nsa.inference.topk import topk_decode\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_decode\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "sm_scale = d ** -0.5\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(len(x_cu_seqlens)-1, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "get_tabels(v[:t2], v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k[:t2], k_cache, y_cu_seqlens)\n",
    "top_n = 16\n",
    "_, lse = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4)\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "topk = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, top_n=top_n)\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "out, _ = slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=False)\n",
    "o1, o2 = fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=False)\n",
    "out2 = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=NSAHelper.x_seqlens, page_table=block_tables, softmax_scale=sm_scale, window_size=(512, -1)).squeeze(1)\n",
    "# \n",
    "compare(o1, out)\n",
    "compare(out2, o2)\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, max_num_splits=1, sm_scale=1.), warmup=100, rep=500))\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, max_num_splits=2, sm_scale=1.), warmup=100, rep=500))\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, max_num_splits=4, sm_scale=1.), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03341905783473525\n",
      "0.008869808155007203\n",
      "0.04254714058604368\n",
      "0.009567977850888846\n"
     ]
    }
   ],
   "source": [
    "out, _ = slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=True)\n",
    "o1, o2 = fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04730390714165489\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=16, num_splits=0, max_num_splits=4, window_size=512, sm_scale=sm_scale, bench=False)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.03125, mean_diff: 0.001861572265625\n",
      "0.036204381221125514\n",
      "0.006887265802364237\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.combine import combine_decode\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "t, qh, d = 32, 64, 128 \n",
    "\n",
    "a = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "b = torch.randn_like(a)\n",
    "c = torch.randn_like(a)\n",
    "w = torch.randn(t, qh, 3, device=device, dtype=dtype)\n",
    "def func():\n",
    "    return a * w[..., 0].unsqueeze(-1).sigmoid() + b * w[..., 1].unsqueeze(-1).sigmoid() + c * w[..., 2].unsqueeze(-1).sigmoid()\n",
    "ref_o = func()\n",
    "o = combine_decode(a, b, c, w)\n",
    "compare(o, ref_o)\n",
    "print(triton.testing.do_bench(lambda: func()))\n",
    "print(triton.testing.do_bench(lambda: combine_decode(a, b, c, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006314778922550511\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    combine_decode(a, b, c, w)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "max_diff: 0.00390625, mean_diff: 0.00013637542724609375\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference import combine_decode, cmp_attn_decode, topk_decode, fused_slc_swa_attn_decode\n",
    "from flash_nsa import slc_topk_indices, compress_attn, select_attn, swa_varlen_func\n",
    "from flash_nsa.ops.combine import fused_sigmoid_combine\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 2, 192\n",
    "vd = 128\n",
    "total_blocks = 2048\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 1024, 512)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:] - 1]\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, vd, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, vd, device=device, dtype=dtype)\n",
    "total_w = torch.randn(t, qh, 3, device=device, dtype=dtype)\n",
    "w = total_w[x_cu_seqlens[1:] - 1]\n",
    "\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "\n",
    "def func1():\n",
    "    get_tabels(v, v_cache, y_cu_seqlens)\n",
    "    block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "    cmp_o, lse = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4, tma=True)\n",
    "    topk = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)\n",
    "    get_tabels(v, v_cache, x_cu_seqlens)\n",
    "    block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "    slc_o, swa_o = fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, context_lens)\n",
    "    o = combine_decode(cmp_o, slc_o, swa_o, w)\n",
    "    return o\n",
    "\n",
    "def func2():\n",
    "    cmp_k = k[:t2]\n",
    "    cmp_v = v[:t2]\n",
    "    # compute compress attention\n",
    "    cmp_o, cmp_lse = compress_attn(total_q, cmp_k, cmp_v)\n",
    "    # compute topk indices\n",
    "    topk, _ = slc_topk_indices(total_q, cmp_k, cmp_lse, maybe_efficient_version=False, scale_slc_p=True, fp32=True)\n",
    "    # compute select attention\n",
    "    slc_o = select_attn(total_q, k, v, topk)\n",
    "    # compute sliding window attention\n",
    "    swa_o = swa_varlen_func(total_q, k, v)\n",
    "    # combine the 3 attention outputs\n",
    "    combine_o = fused_sigmoid_combine(cmp_o, slc_o, swa_o, total_w)\n",
    "    # return cmp_o[x_cu_seqlens[1:] - 1]\n",
    "    return combine_o[x_cu_seqlens[1:] - 1]\n",
    "ref_o = func2()\n",
    "o = func1()\n",
    "\n",
    "compare(o, ref_o)\n",
    "# print(triton.testing.do_bench(lambda: func1()))\n",
    "# print(triton.testing.do_bench(lambda:cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=False), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "1.0116333346813917 0.41985932715458446 0.4875258844474266 2.4094578094460837 2.0750351252180197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from flash_nsa.inference import nsa_decode\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "qh, kh, d = 64, 4, 128\n",
    "c = 1\n",
    "total_blocks = 4096 * 4 * c\n",
    "block_size = 128\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "S = 1024 * 8\n",
    "B = 4 * 1024 ** 3 // (S * kh * 2 * 2 * d) * c\n",
    "print(B)\n",
    "t = B * S\n",
    "x_cu_seqlens = generate_cu_seqlens(t, S, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(B, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "w = torch.randn(B, qh, 3, device=device, dtype=dtype)\n",
    "cmp_slot_mapping = torch.zeros(B, device=device, dtype=torch.int32)\n",
    "\n",
    "get_tabels(v[:t2], v_cache, y_cu_seqlens)\n",
    "cmp_block_tables,_= get_tabels(k[:t2], k_cache, y_cu_seqlens)\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "num_slc_blocks = triton.cdiv(S, 64)\n",
    "fixd_y_maxlen = S // 16\n",
    "o = nsa_decode(q, k_cache, v_cache, w, fixed_num_slc_blocks=num_slc_blocks, fixed_y_maxlen=fixd_y_maxlen, \n",
    "               context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping)\n",
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = nsa_decode(q, k_cache, v_cache, w, fixed_num_slc_blocks=num_slc_blocks, fixed_y_maxlen=fixd_y_maxlen, cmp_num_splits=0, \n",
    "                   context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping)\n",
    "t1 = triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, causal=True))\n",
    "t2 = triton.testing.do_bench(lambda: g.replay(), warmup=20, rep=100)\n",
    "t3 = triton.testing.do_bench(lambda: nsa_decode(q, k_cache, v_cache, w, context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping))\n",
    "print(t1, t2, t3, t1/t2, t1/t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_head=64, num_kv_head=4, head_dim=128, kv memory=4G, decoding:\n",
      "     seqlen       NSA  GRAPH-NSA       FA3\n",
      "0    4096.0  0.700775   0.689377  1.034566\n",
      "1    8192.0  0.433544   0.420754  1.021608\n",
      "2   16384.0  0.360706   0.279387  1.003379\n",
      "3   32768.0  0.348726   0.205926  1.024930\n",
      "4   65536.0  0.349135   0.172448  1.001430\n",
      "5  131072.0  0.355115   0.159892  0.997100\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference import nsa_decode\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 4096 * 4\n",
    "block_size = 128\n",
    "\n",
    "memory = t * kh * d * 2 * 2 / 1024 / 1024 / 1024\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seqlen'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[1024 * i for i in [4, 8, 16, 32, 64, 128]],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['NSA', \"GRAPH-NSA\", 'FA3'],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"NSA\",\n",
    "            \"GRAPH-NSA\",\n",
    "            \"FA3\",\n",
    "        ],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles\n",
    "        ylabel=\"ms\",  # label name for the y-axis\n",
    "        plot_name=\"num_head=64, num_kv_head=4, head_dim=128, kv memory=4G, decoding\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'qh':64, 'kh':4, 'd':128},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(qh, kh, seqlen, d, provider):\n",
    "    device = 'cuda'\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    B = 4 * 1024 * 1024 * 1024 // (seqlen * kh * 2 * 2 * d)\n",
    "    \n",
    "    q = torch.randn(B, qh, d, device=device, dtype=dtype)\n",
    "    w = torch.randn(B, qh, 3, device=device, dtype=dtype)\n",
    "    cmp_slot_mapping = torch.zeros(B, device=device, dtype=torch.int32)\n",
    "    \n",
    "    x_cu_seqlens = generate_cu_seqlens(t, seqlen, 0)\n",
    "    NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "    y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "    t2 = y_cu_seqlens[-1].item()\n",
    "    \n",
    "    cmp_block_tables,_= get_tabels(k[:t2], k_cache, y_cu_seqlens)\n",
    "    block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "    \n",
    "    num_slc_blocks = triton.cdiv(seqlen, 64)\n",
    "    fixd_y_maxlen = seqlen // 16\n",
    "    \n",
    "    stream = torch.cuda.Stream()\n",
    "    torch.cuda.set_stream(stream)\n",
    "    if provider == 'GRAPH-NSA':\n",
    "        \n",
    "        g = torch.cuda.CUDAGraph()\n",
    "        with torch.cuda.graph(g):\n",
    "            o = nsa_decode(q, k_cache, v_cache, w, fixed_y_maxlen=fixd_y_maxlen, fixed_num_slc_blocks=num_slc_blocks,\n",
    "                           context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping)\n",
    "        ms = triton.testing.do_bench(lambda: g.replay())\n",
    "    if provider == 'NSA':\n",
    "        ms = triton.testing.do_bench(lambda: nsa_decode(q, k_cache, v_cache, w, fixed_y_maxlen=fixd_y_maxlen, fixed_num_slc_blocks=num_slc_blocks,\n",
    "                                                        context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping))\n",
    "    if provider == 'FA3':\n",
    "        ms = triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, causal=True))\n",
    "    return ms\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "ffn_size = 12288\n",
    "hidden_size = 4096\n",
    "qkvw_size = (64 + 4 + 4) * 128 + 64 * 3\n",
    "o_size = 64 * 128\n",
    "\n",
    "@torch.compile\n",
    "def silu(x):\n",
    "    gate, up = x.chunk(2, -1)\n",
    "    return torch.nn.functional.silu(gate) * up\n",
    "\n",
    "@torch.compile\n",
    "def rms_forward(\n",
    "    x: torch.Tensor,\n",
    "    weight,\n",
    "    eps=1e-16\n",
    ") -> torch.Tensor:\n",
    "    orig_dtype = x.dtype\n",
    "    x = x.float()\n",
    "    var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "    x.mul_(torch.rsqrt(var))\n",
    "    x = x.to(orig_dtype).mul_(weight)\n",
    "    return x\n",
    "\n",
    "bs = 256\n",
    "fc1 = torch.randn(qkvw_size, hidden_size, device=device, dtype=dtype)\n",
    "fc2 = torch.randn(hidden_size, o_size, device=device, dtype=dtype)\n",
    "fc3 = torch.randn(ffn_size * 2, hidden_size, device=device, dtype=dtype)\n",
    "fc4 = torch.randn(hidden_size, ffn_size, device=device, dtype=dtype)\n",
    "x = torch.randn(bs, hidden_size, device=device, dtype=dtype)\n",
    "q = torch.randn(bs, 64, 128, device=device, dtype=dtype)\n",
    "k = torch.randn(bs, 4, 128, device=device, dtype=dtype)\n",
    "o = torch.randn(bs, 64 * 128, device=device, dtype=dtype)\n",
    "w1 = torch.randn(hidden_size, dtype=dtype, device=device)\n",
    "w2 = torch.randn(128, dtype=dtype, device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def func():\n",
    "    rms_forward(x, w1)\n",
    "    torch.matmul(x, fc1.t())\n",
    "    rms_forward(q, w2)\n",
    "    rms_forward(k, w2)\n",
    "    torch.matmul(o, fc2.t())\n",
    "    rms_forward(x, w1)\n",
    "    gate_up = torch.matmul(x, fc3.t())\n",
    "    down = silu(gate_up)\n",
    "    out = torch.matmul(down, fc4.t())\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22075250152926826\n",
      "0.20584278306721346\n"
     ]
    }
   ],
   "source": [
    "print(triton.testing.do_bench(lambda: func()))\n",
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = func()\n",
    "print(triton.testing.do_bench(lambda: g.replay()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunk_prefill demo\n",
    "\n",
    "I dont know inference a lot, this is just a demo. \n",
    "\n",
    "It's obvious that the nsa kernel can support chunk prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.00042724609375, mean_diff: 1.546140993013978e-11\n",
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference import nsa_prefill\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 8192\n",
    "block_size = 128\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "t = 8192\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 2048, 512)\n",
    "# x_cu_seqlens = torch.tensor([0, 8120, 8192], device=device, dtype=torch.int32)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "seqlens = NSAHelper.x_seqlens\n",
    "x_maxlen = NSAHelper.x_maxlen\n",
    "y_maxlen = NSAHelper.y_maxlen\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "w = torch.randn(t, qh, 3, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(k, k_cache, x_cu_seqlens)\n",
    "block_tables, context_lens, cmp_slot_mapping = get_tabels(v, v_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "cmp_block_tables = block_tables\n",
    "ref_o = nsa_prefill(q, k_cache, v_cache, w, x_cu_seqlens, y_cu_seqlens, x_maxlen, y_maxlen, context_lens=context_lens, cmp_slot_mapping=cmp_slot_mapping, cmp_block_tables=cmp_block_tables, block_tables=block_tables)\n",
    "ref_cmp_k = k_cache.flatten(0, 1)[cmp_slot_mapping]\n",
    "k_cache.flatten(0, 1)[cmp_slot_mapping].zero_()\n",
    "\n",
    "B = len(x_cu_seqlens) - 1\n",
    "chunks = 4\n",
    "starts = x_cu_seqlens[:-1].cpu().tolist()\n",
    "ends = x_cu_seqlens[1:].cpu().tolist()\n",
    "chunk_seqlens = triton.cdiv(seqlens, chunks).cpu().tolist()\n",
    "chunk_context_lens = torch.zeros_like(context_lens)\n",
    "o = torch.zeros_like(ref_o)\n",
    "for i in range(chunks):\n",
    "    chunk_q = []\n",
    "    chunk_w = []\n",
    "    chunk_cmp_slot_mapping = []\n",
    "    chunk_y_cu_seqlens = [0]\n",
    "    chunk_x_cu_seqlens = [0]\n",
    "    chunk_x_maxlen = 0\n",
    "    chunk_y_maxlen = 0\n",
    "    for j in range(B):\n",
    "        chunk_start = starts[j] + i * chunk_seqlens[j]\n",
    "        chunk_end = min(ends[j], chunk_start + chunk_seqlens[j])\n",
    "        x_len = chunk_end - chunk_start\n",
    "        old_y_len = max((chunk_start - starts[j] - 32) // 16 + 1, 0)\n",
    "        y_len = max((chunk_end - starts[j] - 32) // 16 + 1, 0)\n",
    "        chunk_cmp_slot_mapping += [-1] * old_y_len\n",
    "        chunk_cmp_slot_mapping += cmp_slot_mapping[y_cu_seqlens[j] + old_y_len: y_cu_seqlens[j] + y_len].cpu().tolist()\n",
    "        chunk_x_cu_seqlens.append(chunk_x_cu_seqlens[-1] + x_len)\n",
    "        chunk_y_cu_seqlens.append(chunk_y_cu_seqlens[-1] + y_len)\n",
    "        chunk_x_maxlen = max(chunk_x_maxlen, x_len)\n",
    "        chunk_y_maxlen = max(chunk_y_maxlen, y_len)\n",
    "        chunk_q.append(q[chunk_start:chunk_end])\n",
    "        chunk_w.append(w[chunk_start:chunk_end])\n",
    "        chunk_context_lens[j] += x_len\n",
    "    chunk_q = torch.cat(chunk_q, 0)\n",
    "    chunk_w = torch.cat(chunk_w, 0)\n",
    "    chunk_x_cu_seqlens = torch.tensor(chunk_x_cu_seqlens, device=device, dtype=torch.int32)\n",
    "    chunk_y_cu_seqlens = torch.tensor(chunk_y_cu_seqlens, device=device, dtype=torch.int32)\n",
    "    chunk_cmp_slot_mapping = torch.tensor(chunk_cmp_slot_mapping, device=device, dtype=torch.int32)\n",
    "    # chunk_cmp_slot_mapping, chunk_context_lens and chunk_y_cu_seqlens need pre chunks info\n",
    "    chunk_o = nsa_prefill(chunk_q, k_cache, v_cache, chunk_w, chunk_x_cu_seqlens, chunk_y_cu_seqlens, chunk_x_maxlen, chunk_y_maxlen, \n",
    "                          context_lens=chunk_context_lens, cmp_slot_mapping=chunk_cmp_slot_mapping, cmp_block_tables=cmp_block_tables, block_tables=block_tables)\n",
    "    bos = 0\n",
    "    for j in range(B):\n",
    "        chunk_start = starts[j] + i * chunk_seqlens[j]\n",
    "        chunk_end = min(ends[j], chunk_start + chunk_seqlens[j])\n",
    "        o[chunk_start:chunk_end] = chunk_o[bos:bos+chunk_end-chunk_start]\n",
    "        bos += chunk_end - chunk_start\n",
    "cmp_k = k_cache.flatten(0, 1)[cmp_slot_mapping]\n",
    "compare(o, ref_o)\n",
    "compare(cmp_k, ref_cmp_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
