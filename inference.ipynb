{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129\n",
      "3.4.0\n",
      "2.8.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maduyue/code/repo/Scalable-Flash-Native-Sparse-Attention/flash_nsa/utils.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import random\n",
    "import os\n",
    "os.environ['TRITON_PRINT_AUTOTUNING'] = '1'\n",
    "import flash_attn\n",
    "print(torch.__version__)\n",
    "print(triton.__version__)\n",
    "print(flash_attn.__version__)\n",
    "def compare(x, y, prefix=\"\"):\n",
    "    if x is None or y is None:\n",
    "        return\n",
    "    assert x.shape == y.shape\n",
    "    if 0 in list(x.shape):\n",
    "        return\n",
    "    \n",
    "    if any([x.dtype == torch.float32, y.dtype==torch.float32]):\n",
    "        x,y = x.float(), y.float()\n",
    "    diff = (x-y).abs()\n",
    "    if prefix:\n",
    "        print(prefix, end=\": \")\n",
    "    print(f\"max_diff: {diff.max().item()}, mean_diff: {diff.mean().item()}\")\n",
    "\n",
    "def generate_cu_seqlens(end=8192, mean=2048, std=512):\n",
    "    r = [0]\n",
    "    while r[-1] < end:\n",
    "        a = random.randint(mean-std, mean+std)\n",
    "        r.append(r[-1] + a)\n",
    "    r[-1] = end\n",
    "    cu_seqlens = torch.tensor(r, device=torch.cuda.current_device(), dtype=torch.int32)\n",
    "    return cu_seqlens\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_tabels(x, y, cu_seqlens, seed=42, return_cmp_slot_mapping=False):\n",
    "    B = len(cu_seqlens) - 1\n",
    "    total_num_blocks, block_size, H, D = y.shape\n",
    "    y = y.view(-1, H, D)\n",
    "    \n",
    "    unused = np.arange(0, total_num_blocks)\n",
    "    # np.random.seed(seed)\n",
    "    # np.random.shuffle(unused)\n",
    "    block_tabels = []\n",
    "    context_lens = []\n",
    "    cmp_slop_mapping = []\n",
    "    off = 0\n",
    "    for i in range(B):\n",
    "        s = (cu_seqlens[i+1] - cu_seqlens[i]).item()\n",
    "        context_lens.append(s)\n",
    "        need = triton.cdiv(s, block_size)\n",
    "        block_ids = unused[off:off+need]\n",
    "        block_tabels.append(block_ids.tolist())\n",
    "        slot = block_ids[:, None] * block_size + np.arange(0, block_size)[None, :]\n",
    "        slot = slot.reshape(-1)[:s]\n",
    "        slot = torch.tensor(slot, device=x.device)\n",
    "        y[slot] = x[cu_seqlens[i]:cu_seqlens[i+1]]\n",
    "        off += need\n",
    "        if return_cmp_slot_mapping:\n",
    "            kernel_size = NSAHelper.kernel_size\n",
    "            stride = NSAHelper.stride\n",
    "            cmp_len = max((s - kernel_size) // stride + 1, 0)\n",
    "            need = triton.cdiv(cmp_len, block_size)\n",
    "            block_ids = unused[off:off+need]\n",
    "            slot = block_ids[:, None] * block_size + np.arange(0, block_size)[None, :]\n",
    "            slot = slot.reshape(-1)[:cmp_len]\n",
    "            cmp_slop_mapping.extend(slot.tolist())\n",
    "            off += need\n",
    "    maxlen = triton.cdiv(max([len(i) for i in block_tabels]), 8) * 8\n",
    "    block_tabels = [i + [-1] * (maxlen - len(i)) for i in block_tabels]\n",
    "    block_tabels = torch.tensor(block_tabels, device=x.device, dtype=torch.int32)\n",
    "    context_lens = torch.tensor(context_lens, device=x.device, dtype=torch.int32)\n",
    "    \n",
    "    if not return_cmp_slot_mapping:\n",
    "        return block_tabels, context_lens\n",
    "    else:\n",
    "        cmp_slop_mapping = torch.tensor(cmp_slop_mapping, device=x.device, dtype=torch.int32)\n",
    "        return block_tabels, context_lens, cmp_slop_mapping\n",
    "from flash_nsa import NSAHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.001953125, mean_diff: 3.9577484130859375e-05\n",
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_attn_interface import flash_attn_varlen_func, flash_attn_with_kvcache\n",
    "from flash_attn import flash_attn_with_kvcache as flash_attn_with_kvcache_v2\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 128\n",
    "block_size = 128\n",
    "\n",
    "cu_seqlens_k = generate_cu_seqlens(t, 513, 0)\n",
    "cu_seqlens_q = torch.arange(len(cu_seqlens_k), device=cu_seqlens_k.device, dtype=torch.int32)\n",
    "maxlen = (cu_seqlens_k[1:] - cu_seqlens_k[:-1]).max().item()\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[cu_seqlens_k[1:]-1]\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, cu_seqlens_k)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, cu_seqlens_k)\n",
    "window_size = (512, -1)\n",
    "out1 = flash_attn_varlen_func(total_q, k, v, cu_seqlens_k, cu_seqlens_k, maxlen, maxlen, \n",
    "                            #   window_size=window_size, \n",
    "                              causal=True)\n",
    "out2 = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, window_size=window_size)\n",
    "out3 = flash_attn_with_kvcache(total_q, k_cache, v_cache, cu_seqlens_q=cu_seqlens_k, max_seqlen_q=maxlen, cache_seqlens=context_lens, page_table=block_tables, causal=True, window_size=window_size)\n",
    "compare(out1[cu_seqlens_k[1:]-1], out2.squeeze(1))\n",
    "compare(out1, out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn_interface import flash_attn_varlen_func, flash_attn_with_kvcache\n",
    "from flash_attn import flash_attn_with_kvcache as flash_attn_with_kvcache_v2\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64 * 4\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 10\n",
    "block_size = 256\n",
    "\n",
    "B = 32\n",
    "S = 16 * 1024\n",
    "t = B * S\n",
    "cu_seqlens_k = generate_cu_seqlens(t, S, 0)\n",
    "cu_seqlens_q = torch.arange(len(cu_seqlens_k), device=cu_seqlens_k.device, dtype=torch.int32)\n",
    "maxlen = (cu_seqlens_k[1:] - cu_seqlens_k[:-1]).max().item()\n",
    "q = torch.randn(len(cu_seqlens_k) - 1, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, cu_seqlens_k)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, cu_seqlens_k)\n",
    "window_size = (512, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25801832703026856\n",
      "0.26085690104693077\n",
      "0.027003388910088688\n",
      "0.16595018988820762\n"
     ]
    }
   ],
   "source": [
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables)))\n",
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache_v2(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, block_table=block_tables)))\n",
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, window_size=window_size)))\n",
    "print(triton.testing.do_bench(lambda: flash_attn_with_kvcache_v2(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, block_table=block_tables, window_size=window_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25929387067268844\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compress kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.0, mean_diff: 0.0\n",
      "0.009867040589073763\n",
      "0.010524349875664736\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import mean_pooling\n",
    "from flash_nsa.inference.compress_kv import mean_pooling_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, t//4, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.randn(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.randn(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "block_tables, context_lens, cmp_slot_mapping = get_tabels(v, v_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "ref_out = mean_pooling(v)\n",
    "\n",
    "mean_pooling_prefill(v_cache, y_cu_seqlens, NSAHelper.y_maxlen, cmp_slot_mapping, block_tables)\n",
    "out = v_cache.flatten(0, 1)[cmp_slot_mapping]\n",
    "compare(ref_out, out)\n",
    "print(triton.testing.do_bench(lambda: mean_pooling(v)))\n",
    "print(triton.testing.do_bench(lambda: mean_pooling_prefill(v_cache, y_cu_seqlens, NSAHelper.y_maxlen, cmp_slot_mapping, block_tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "max_diff: 0.0, mean_diff: 0.0\n",
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.compress_kv import mean_pooling_decode, kv_mean_pooling_decode\n",
    "from flash_nsa import mean_pooling\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 8192\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 128\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 512, 0)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "get_tabels(v, v_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "block_tables, context_lens, cmp_slop_mapping = get_tabels(k, k_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "# must be not -1 in test\n",
    "cmp_slop_mapping = cmp_slop_mapping[y_cu_seqlens[1:] - 1]\n",
    "\n",
    "ref_cmpk = mean_pooling(k)[y_cu_seqlens[1:] - 1]\n",
    "ref_cmpv = mean_pooling(v)[y_cu_seqlens[1:] - 1]\n",
    "kv_mean_pooling_decode(k_cache, v_cache, cmp_slop_mapping, block_tables, context_lens)\n",
    "cmpk = k_cache.flatten(0, 1)[cmp_slop_mapping]\n",
    "cmpv = v_cache.flatten(0, 1)[cmp_slop_mapping]\n",
    "compare(ref_cmpk, cmpk)\n",
    "compare(ref_cmpv, cmpv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "0.010033327620476484\n",
      "0.012787732492674658\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.compress_kv import mean_pooling_decode, kv_mean_pooling_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 8 * 1024, 0)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "cmp_slop_mapping = torch.randint(0, total_blocks * block_size, (B,), device=device, dtype=torch.int32)\n",
    "a = mean_pooling_decode(k_cache, cmp_slop_mapping, block_tables, context_lens)\n",
    "print(triton.testing.do_bench(lambda: mean_pooling_decode(k_cache, cmp_slop_mapping, block_tables, context_lens)))\n",
    "print(triton.testing.do_bench(lambda: kv_mean_pooling_decode(k_cache, v_cache, cmp_slop_mapping, block_tables, context_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009379466746604021\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = mean_pooling_decode(k_cache, cmp_slop_mapping, block_tables, context_lens)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cmp_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 10248\n",
    "block_size = 256\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "y1,lse1 = compress_attn(q, k, v)\n",
    "y2,lse2 = cmp_attn_prefill(q, k_cache, v_cache, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)\n",
    "compare(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.047578204761852\n",
      "8.857786351984197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(triton.testing.do_bench(lambda: cmp_attn_prefill(q, k_cache, v_cache, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)))\n",
    "print(triton.testing.do_bench(lambda: compress_attn(q, k, v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "max_diff: 0.0078125, mean_diff: 0.0002193450927734375\n",
      "max_diff: 9.5367431640625e-07, mean_diff: 1.3560057254835556e-07\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_prefill, cmp_attn_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 8192\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 2048, 512)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:]-1]\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "y1,lse1 = compress_attn(total_q, k, v)\n",
    "y1 = y1[x_cu_seqlens[1:]-1]\n",
    "lse1 = lse1[:, x_cu_seqlens[1:]-1]\n",
    "y2,lse2 = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=0)\n",
    "compare(y1, y2)\n",
    "compare(lse1, lse2.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "0.07877120887664725\n",
      "0.008280133809894324\n",
      "0.08372627695019429\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_prefill, cmp_attn_decode\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:]-1]\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "# y1,_ = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=True)\n",
    "# y2,_ = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=False)\n",
    "# compare(y1, y2)\n",
    "cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=True, bench=True)\n",
    "print(triton.testing.do_bench(lambda:cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=True), warmup=100, rep=100))\n",
    "\n",
    "# print(triton.testing.do_bench(lambda:cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=False), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08243841872495764\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=0, tma=True, bench=False)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "7.129031401414138\n",
      "6.519980843861898\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices\n",
    "from flash_nsa.inference.topk import topk_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 4096\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "# x_cu_seqlens = generate_cu_seqlens(t, 32, 24)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, lse = compress_attn(q, k, v)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "topk1, _ = slc_topk_indices(q, k, lse)\n",
    "topk2 = topk_prefill(q, k_cache, lse, x_cu_seqlens, NSAHelper.x_maxlen, NSAHelper.y_maxlen, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=num_slc_blocks)\n",
    "topk1.masked_fill_(topk1==topk1.max(), -1)\n",
    "print((topk1 != topk2).sum())\n",
    "print(triton.testing.do_bench(lambda: slc_topk_indices(q, k, lse)))\n",
    "print(triton.testing.do_bench(lambda: topk_prefill(q, k_cache, lse, x_cu_seqlens, NSAHelper.x_maxlen, NSAHelper.y_maxlen, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=num_slc_blocks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices\n",
    "from flash_nsa.inference.topk import topk_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 128\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 1024, 512)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:] - 1]\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, total_lse = compress_attn(total_q, k, v)\n",
    "lse = total_lse.transpose(0,1)[x_cu_seqlens[1:] - 1]\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "topk1, _ = slc_topk_indices(total_q, k, total_lse, ignore_index=2048, maybe_efficient_version=False, fp32=True)\n",
    "topk1.masked_fill_(topk1==topk1.max(), -1)\n",
    "topk1 = topk1[:, x_cu_seqlens[1:] - 1]\n",
    "\n",
    "topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)\n",
    "print((topk1 != topk2).sum())\n",
    "# print(triton.testing.do_bench(lambda: slc_topk_indices(q, k, lse)))\n",
    "# print(triton.testing.do_bench(lambda: topk_prefill(q, k_cache, lse, x_cu_seqlens, NSAHelper.x_maxlen, NSAHelper.y_maxlen, block_tables, NSAHelper.x_seqlens, num_slc_blocks=num_slc_blocks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "0.05582140394122593\n",
      "0.04515248586357028\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.topk import topk_decode\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 8 * 1024, 512)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(len(x_cu_seqlens)-1, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t2, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(v, v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "\n",
    "_, lse = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4)\n",
    "topk1 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)\n",
    "topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, persistent=True, workers=4)\n",
    "print((topk1-topk2).sum())\n",
    "print(triton.testing.do_bench(lambda: cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=2)))\n",
    "print(triton.testing.do_bench(lambda: topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03993252619965145\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=2048, fixed_y_maxlen=8192)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033605471474296\n"
     ]
    }
   ],
   "source": [
    "g2 = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g2):\n",
    "    topk2 = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, fixed_num_slc_blocks=2048, persistent=True, workers=4)\n",
    "print(triton.testing.do_bench(lambda: g2.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# slc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.0, mean_diff: 0.0\n",
      "13.33336693899972\n",
      "13.311012676783971\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices, select_attn\n",
    "from flash_nsa.inference.select_attn import slc_attn_prefill\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 64\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1280\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, lse = compress_attn(q, k[:t2], v[:t2])\n",
    "\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "topk, _ = slc_topk_indices(q, k[:t2], lse)\n",
    "out1 = select_attn(q, k, v, topk)\n",
    "out2, _ = slc_attn_prefill(q, k_cache, v_cache, topk, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)\n",
    "compare(out1, out2)\n",
    "# print((topk1 != topk2).sum())\n",
    "print(triton.testing.do_bench(lambda: select_attn(q, k, v, topk)))\n",
    "print(triton.testing.do_bench(lambda: slc_attn_prefill(q, k_cache, v_cache, topk, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.001953125, mean_diff: 6.67572021484375e-05\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa import compress_attn, slc_topk_indices, select_attn\n",
    "from flash_nsa.inference.select_attn import slc_attn_decode\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1280\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 1024, 512)\n",
    "# x_cu_seqlens = generate_cu_seqlens(t, t, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:]-1]\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "_, lse = compress_attn(total_q, k[:t2], v[:t2])\n",
    "\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "total_topk, _ = slc_topk_indices(total_q, k[:t2], lse)\n",
    "topk = total_topk[:, x_cu_seqlens[1:]-1]\n",
    "# topk = topk.sort(-1)[0]\n",
    "out1 = select_attn(total_q, k, v, total_topk)\n",
    "out1 = out1[x_cu_seqlens[1:]-1]\n",
    "out2, _ = slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens)\n",
    "compare(out1, out2)\n",
    "# print((topk1 != topk2).sum())\n",
    "# print(triton.testing.do_bench(lambda: select_attn(q, k, v, topk)))\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_prefill(q, k_cache, v_cache, topk, x_cu_seqlens, NSAHelper.x_maxlen, block_tables, NSAHelper.x_seqlens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.00048828125, mean_diff: 2.9831426218152046e-09\n",
      "max_diff: 0.001953125, mean_diff: 9.918212890625e-05\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.select_attn import slc_attn_decode, fused_slc_swa_attn_decode\n",
    "from flash_nsa.inference.topk import topk_decode\n",
    "from flash_nsa.inference.compress_attn import cmp_attn_decode\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 1024 * 36\n",
    "block_size = 128\n",
    "sm_scale = d ** -0.5\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 64 * 1024, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(len(x_cu_seqlens)-1, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "get_tabels(v[:t2], v_cache, y_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k[:t2], k_cache, y_cu_seqlens)\n",
    "top_n = 16\n",
    "_, lse = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4)\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "topk = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens, top_n=top_n)\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "out, _ = slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=False)\n",
    "o1, o2 = fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=False)\n",
    "out2 = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=NSAHelper.x_seqlens, page_table=block_tables, softmax_scale=sm_scale, window_size=(512, -1)).squeeze(1)\n",
    "# \n",
    "compare(o1, out)\n",
    "compare(out2, o2)\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, max_num_splits=1, sm_scale=1.), warmup=100, rep=500))\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, max_num_splits=2, sm_scale=1.), warmup=100, rep=500))\n",
    "# print(triton.testing.do_bench(lambda: slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, max_num_splits=4, sm_scale=1.), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03341905783473525\n",
      "0.008869808155007203\n",
      "0.04254714058604368\n",
      "0.009567977850888846\n"
     ]
    }
   ],
   "source": [
    "out, _ = slc_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=True)\n",
    "o1, o2 = fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=top_n, num_splits=4, sm_scale=sm_scale, bench=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04730390714165489\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, NSAHelper.x_seqlens, top_n=16, num_splits=0, max_num_splits=4, window_size=512, sm_scale=sm_scale, bench=False)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.03125, mean_diff: 0.001861572265625\n",
      "0.036204381221125514\n",
      "0.006887265802364237\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference.combine import combine_decode\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "t, qh, d = 32, 64, 128 \n",
    "\n",
    "a = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "b = torch.randn_like(a)\n",
    "c = torch.randn_like(a)\n",
    "w = torch.randn(t, qh, 3, device=device, dtype=dtype)\n",
    "def func():\n",
    "    return a * w[..., 0].unsqueeze(-1).sigmoid() + b * w[..., 1].unsqueeze(-1).sigmoid() + c * w[..., 2].unsqueeze(-1).sigmoid()\n",
    "ref_o = func()\n",
    "o = combine_decode(a, b, c, w)\n",
    "compare(o, ref_o)\n",
    "print(triton.testing.do_bench(lambda: func()))\n",
    "print(triton.testing.do_bench(lambda: combine_decode(a, b, c, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006314778922550511\n"
     ]
    }
   ],
   "source": [
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    combine_decode(a, b, c, w)\n",
    "print(triton.testing.do_bench(lambda: g.replay(), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "max_diff: 0.00390625, mean_diff: 9.822845458984375e-05\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference import combine_decode, cmp_attn_decode, topk_decode, fused_slc_swa_attn_decode\n",
    "from flash_nsa import slc_topk_indices, compress_attn, select_attn, swa_varlen_func\n",
    "from flash_nsa.ops.combine import fused_sigmoid_combine\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "t = 1024 * 8\n",
    "qh, kh, d = 64, 2, 192\n",
    "vd = 128\n",
    "total_blocks = 2048\n",
    "block_size = 128\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 1024, 512)\n",
    "B = len(x_cu_seqlens) - 1\n",
    "print(B)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "total_q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "q = total_q[x_cu_seqlens[1:] - 1]\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, vd, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, vd, device=device, dtype=dtype)\n",
    "total_w = torch.randn(t, qh, 3, device=device, dtype=dtype)\n",
    "w = total_w[x_cu_seqlens[1:] - 1]\n",
    "\n",
    "num_slc_blocks = triton.cdiv(NSAHelper.x_maxlen, NSAHelper.block_size)\n",
    "\n",
    "def func1():\n",
    "    get_tabels(v, v_cache, y_cu_seqlens)\n",
    "    block_tables, context_lens = get_tabels(k, k_cache, y_cu_seqlens)\n",
    "    cmp_o, lse = cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, num_splits=4, tma=True)\n",
    "    topk = topk_decode(q, k_cache, lse, block_tables, NSAHelper.x_seqlens)\n",
    "    get_tabels(v, v_cache, x_cu_seqlens)\n",
    "    block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "    slc_o, swa_o = fused_slc_swa_attn_decode(q, k_cache, v_cache, topk, block_tables, context_lens)\n",
    "    o = combine_decode(cmp_o, slc_o, swa_o, w)\n",
    "    return o\n",
    "\n",
    "def func2():\n",
    "    cmp_k = k[:t2]\n",
    "    cmp_v = v[:t2]\n",
    "    # compute compress attention\n",
    "    cmp_o, cmp_lse = compress_attn(total_q, cmp_k, cmp_v)\n",
    "    # compute topk indices\n",
    "    topk, _ = slc_topk_indices(total_q, cmp_k, cmp_lse, maybe_efficient_version=False, scale_slc_p=True, fp32=True)\n",
    "    # compute select attention\n",
    "    slc_o = select_attn(total_q, k, v, topk)\n",
    "    # compute sliding window attention\n",
    "    swa_o = swa_varlen_func(total_q, k, v)\n",
    "    # combine the 3 attention outputs\n",
    "    combine_o = fused_sigmoid_combine(cmp_o, slc_o, swa_o, total_w)\n",
    "    # return cmp_o[x_cu_seqlens[1:] - 1]\n",
    "    return combine_o[x_cu_seqlens[1:] - 1]\n",
    "ref_o = func2()\n",
    "o = func1()\n",
    "\n",
    "compare(o, ref_o)\n",
    "# print(triton.testing.do_bench(lambda: func1()))\n",
    "# print(triton.testing.do_bench(lambda:cmp_attn_decode(q, k_cache, v_cache, block_tables, NSAHelper.x_seqlens, tma=False), warmup=100, rep=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "1.0044039972126484 0.4205244177609534 0.5398580745443121 2.3884558298909546 1.8604963870559017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from flash_nsa.inference import nsa_decode\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "qh, kh, d = 64, 4, 128\n",
    "c = 1\n",
    "total_blocks = 4096 * 4 * c\n",
    "block_size = 128\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "S = 1024 * 8\n",
    "B = 4 * 1024 ** 3 // (S * kh * 2 * 2 * d) * c\n",
    "print(B)\n",
    "t = B * S\n",
    "x_cu_seqlens = generate_cu_seqlens(t, S, 0)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(B, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "w = torch.randn(B, qh, 3, device=device, dtype=dtype)\n",
    "cmp_slot_mapping = torch.zeros(B, device=device, dtype=torch.int32)\n",
    "\n",
    "get_tabels(v[:t2], v_cache, y_cu_seqlens)\n",
    "cmp_block_tables,_= get_tabels(k[:t2], k_cache, y_cu_seqlens)\n",
    "get_tabels(v, v_cache, x_cu_seqlens)\n",
    "block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "\n",
    "num_slc_blocks = triton.cdiv(S, 64)\n",
    "fixd_y_maxlen = S // 16\n",
    "o = nsa_decode(q, k_cache, v_cache, w, fixed_num_slc_blocks=num_slc_blocks, fixed_y_maxlen=fixd_y_maxlen, \n",
    "               context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping)\n",
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = nsa_decode(q, k_cache, v_cache, w, fixed_num_slc_blocks=num_slc_blocks, fixed_y_maxlen=fixd_y_maxlen, cmp_num_splits=0, \n",
    "                   context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping)\n",
    "t1 = triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, causal=True))\n",
    "t2 = triton.testing.do_bench(lambda: g.replay(), warmup=20, rep=100)\n",
    "t3 = triton.testing.do_bench(lambda: nsa_decode(q, k_cache, v_cache, w, context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping))\n",
    "print(t1, t2, t3, t1/t2, t1/t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_nsa.inference import nsa_decode\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "t = 1024 * 2048\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 4096 * 4\n",
    "block_size = 128\n",
    "\n",
    "memory = t * kh * d * 2 * 2 / 1024 / 1024 / 1024\n",
    "\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seqlen'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[1024 * i for i in [4, 8, 16, 32, 64, 128]],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['NSA', \"GRAPH-NSA\", 'FA3'],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"NSA\",\n",
    "            \"GRAPH-NSA\",\n",
    "            \"FA3\",\n",
    "        ],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-'), ('red', '-')],  # line styles\n",
    "        ylabel=\"ms\",  # label name for the y-axis\n",
    "        plot_name=\"num_head=64, num_kv_head=4, head_dim=128, kv memory=4G, decoding\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'qh':64, 'kh':4, 'd':128},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(qh, kh, seqlen, d, provider):\n",
    "    device = 'cuda'\n",
    "    dtype = torch.bfloat16\n",
    "    \n",
    "    B = 4 * 1024 * 1024 * 1024 // (seqlen * kh * 2 * 2 * d)\n",
    "    \n",
    "    q = torch.randn(B, qh, d, device=device, dtype=dtype)\n",
    "    w = torch.randn(B, qh, 3, device=device, dtype=dtype)\n",
    "    cmp_slot_mapping = torch.zeros(B, device=device, dtype=torch.int32)\n",
    "    \n",
    "    x_cu_seqlens = generate_cu_seqlens(t, seqlen, 0)\n",
    "    NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "    y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "    t2 = y_cu_seqlens[-1].item()\n",
    "    \n",
    "    cmp_block_tables,_= get_tabels(k[:t2], k_cache, y_cu_seqlens)\n",
    "    block_tables, context_lens = get_tabels(k, k_cache, x_cu_seqlens)\n",
    "    \n",
    "    num_slc_blocks = triton.cdiv(seqlen, 64)\n",
    "    fixd_y_maxlen = seqlen // 16\n",
    "    \n",
    "    stream = torch.cuda.Stream()\n",
    "    torch.cuda.set_stream(stream)\n",
    "    if provider == 'GRAPH-NSA':\n",
    "        \n",
    "        g = torch.cuda.CUDAGraph()\n",
    "        with torch.cuda.graph(g):\n",
    "            o = nsa_decode(q, k_cache, v_cache, w, fixed_y_maxlen=fixd_y_maxlen, fixed_num_slc_blocks=num_slc_blocks,\n",
    "                           context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping)\n",
    "        ms = triton.testing.do_bench(lambda: g.replay())\n",
    "    if provider == 'NSA':\n",
    "        ms = triton.testing.do_bench(lambda: nsa_decode(q, k_cache, v_cache, w, fixed_y_maxlen=fixd_y_maxlen, fixed_num_slc_blocks=num_slc_blocks,\n",
    "                                                        context_lens=context_lens, block_tables=block_tables, cmp_block_tables=cmp_block_tables, cmp_slot_mapping=cmp_slot_mapping))\n",
    "    if provider == 'FA3':\n",
    "        ms = triton.testing.do_bench(lambda: flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache, cache_seqlens=context_lens, page_table=block_tables, causal=True))\n",
    "    return ms\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "ffn_size = 12288\n",
    "hidden_size = 4096\n",
    "qkvw_size = (64 + 4 + 4) * 128 + 64 * 3\n",
    "o_size = 64 * 128\n",
    "\n",
    "@torch.compile\n",
    "def silu(x):\n",
    "    gate, up = x.chunk(2, -1)\n",
    "    return torch.nn.functional.silu(gate) * up\n",
    "\n",
    "@torch.compile\n",
    "def rms_forward(\n",
    "    x: torch.Tensor,\n",
    "    weight,\n",
    "    eps=1e-16\n",
    ") -> torch.Tensor:\n",
    "    orig_dtype = x.dtype\n",
    "    x = x.float()\n",
    "    var = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "    x.mul_(torch.rsqrt(var))\n",
    "    x = x.to(orig_dtype).mul_(weight)\n",
    "    return x\n",
    "\n",
    "bs = 256\n",
    "fc1 = torch.randn(qkvw_size, hidden_size, device=device, dtype=dtype)\n",
    "fc2 = torch.randn(hidden_size, o_size, device=device, dtype=dtype)\n",
    "fc3 = torch.randn(ffn_size * 2, hidden_size, device=device, dtype=dtype)\n",
    "fc4 = torch.randn(hidden_size, ffn_size, device=device, dtype=dtype)\n",
    "x = torch.randn(bs, hidden_size, device=device, dtype=dtype)\n",
    "q = torch.randn(bs, 64, 128, device=device, dtype=dtype)\n",
    "k = torch.randn(bs, 4, 128, device=device, dtype=dtype)\n",
    "o = torch.randn(bs, 64 * 128, device=device, dtype=dtype)\n",
    "w1 = torch.randn(hidden_size, dtype=dtype, device=device)\n",
    "w2 = torch.randn(128, dtype=dtype, device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def func():\n",
    "    rms_forward(x, w1)\n",
    "    torch.matmul(x, fc1.t())\n",
    "    rms_forward(q, w2)\n",
    "    rms_forward(k, w2)\n",
    "    torch.matmul(o, fc2.t())\n",
    "    rms_forward(x, w1)\n",
    "    gate_up = torch.matmul(x, fc3.t())\n",
    "    down = silu(gate_up)\n",
    "    out = torch.matmul(down, fc4.t())\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22075250152926826\n",
      "0.20584278306721346\n"
     ]
    }
   ],
   "source": [
    "print(triton.testing.do_bench(lambda: func()))\n",
    "g = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(g):\n",
    "    o = func()\n",
    "print(triton.testing.do_bench(lambda: g.replay()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunk_prefill demo\n",
    "\n",
    "I dont know inference a lot, this is just a demo. \n",
    "\n",
    "It's obvious that the nsa kernel can support chunk prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: 0.00042724609375, mean_diff: 1.546140993013978e-11\n",
      "max_diff: 0.0, mean_diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from flash_nsa.inference import nsa_prefill\n",
    "from flash_attn_interface import flash_attn_with_kvcache\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "qh, kh, d = 64, 4, 128\n",
    "total_blocks = 8192\n",
    "block_size = 128\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "t = 8192\n",
    "\n",
    "x_cu_seqlens = generate_cu_seqlens(t, 2048, 512)\n",
    "# x_cu_seqlens = torch.tensor([0, 8120, 8192], device=device, dtype=torch.int32)\n",
    "NSAHelper.init_cu_seqlens(x_cu_seqlens)\n",
    "seqlens = NSAHelper.x_seqlens\n",
    "x_maxlen = NSAHelper.x_maxlen\n",
    "y_maxlen = NSAHelper.y_maxlen\n",
    "y_cu_seqlens = NSAHelper.y_cu_seqlens\n",
    "t2 = y_cu_seqlens[-1].item()\n",
    "\n",
    "q = torch.randn(t, qh, d, device=device, dtype=dtype)\n",
    "k = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "v = torch.randn(t, kh, d, device=device, dtype=dtype)\n",
    "k_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "v_cache = torch.empty(total_blocks, block_size, kh, d, device=device, dtype=dtype)\n",
    "w = torch.randn(t, qh, 3, device=device, dtype=dtype)\n",
    "\n",
    "get_tabels(k, k_cache, x_cu_seqlens)\n",
    "block_tables, context_lens, cmp_slot_mapping = get_tabels(v, v_cache, x_cu_seqlens, return_cmp_slot_mapping=True)\n",
    "cmp_block_tables = block_tables\n",
    "ref_o = nsa_prefill(q, k_cache, v_cache, w, x_cu_seqlens, y_cu_seqlens, x_maxlen, y_maxlen, context_lens=context_lens, cmp_slot_mapping=cmp_slot_mapping, cmp_block_tables=cmp_block_tables, block_tables=block_tables)\n",
    "ref_cmp_k = k_cache.flatten(0, 1)[cmp_slot_mapping]\n",
    "k_cache.flatten(0, 1)[cmp_slot_mapping].zero_()\n",
    "\n",
    "B = len(x_cu_seqlens) - 1\n",
    "chunks = 4\n",
    "starts = x_cu_seqlens[:-1].cpu().tolist()\n",
    "ends = x_cu_seqlens[1:].cpu().tolist()\n",
    "chunk_seqlens = triton.cdiv(seqlens, chunks).cpu().tolist()\n",
    "chunk_context_lens = torch.zeros_like(context_lens)\n",
    "o = torch.zeros_like(ref_o)\n",
    "for i in range(chunks):\n",
    "    chunk_q = []\n",
    "    chunk_w = []\n",
    "    chunk_cmp_slot_mapping = []\n",
    "    chunk_y_cu_seqlens = [0]\n",
    "    chunk_x_cu_seqlens = [0]\n",
    "    chunk_x_maxlen = 0\n",
    "    chunk_y_maxlen = 0\n",
    "    for j in range(B):\n",
    "        chunk_start = starts[j] + i * chunk_seqlens[j]\n",
    "        chunk_end = min(ends[j], chunk_start + chunk_seqlens[j])\n",
    "        x_len = chunk_end - chunk_start\n",
    "        old_y_len = max((chunk_start - starts[j] - 32) // 16 + 1, 0)\n",
    "        y_len = max((chunk_end - starts[j] - 32) // 16 + 1, 0)\n",
    "        chunk_cmp_slot_mapping += [-1] * old_y_len\n",
    "        chunk_cmp_slot_mapping += cmp_slot_mapping[y_cu_seqlens[j] + old_y_len: y_cu_seqlens[j] + y_len].cpu().tolist()\n",
    "        chunk_x_cu_seqlens.append(chunk_x_cu_seqlens[-1] + x_len)\n",
    "        chunk_y_cu_seqlens.append(chunk_y_cu_seqlens[-1] + y_len)\n",
    "        chunk_x_maxlen = max(chunk_x_maxlen, x_len)\n",
    "        chunk_y_maxlen = max(chunk_y_maxlen, y_len)\n",
    "        chunk_q.append(q[chunk_start:chunk_end])\n",
    "        chunk_w.append(w[chunk_start:chunk_end])\n",
    "        chunk_context_lens[j] += x_len\n",
    "    chunk_q = torch.cat(chunk_q, 0)\n",
    "    chunk_w = torch.cat(chunk_w, 0)\n",
    "    chunk_x_cu_seqlens = torch.tensor(chunk_x_cu_seqlens, device=device, dtype=torch.int32)\n",
    "    chunk_y_cu_seqlens = torch.tensor(chunk_y_cu_seqlens, device=device, dtype=torch.int32)\n",
    "    chunk_cmp_slot_mapping = torch.tensor(chunk_cmp_slot_mapping, device=device, dtype=torch.int32)\n",
    "    # chunk_cmp_slot_mapping, chunk_context_lens and chunk_y_cu_seqlens need pre chunks info\n",
    "    chunk_o = nsa_prefill(chunk_q, k_cache, v_cache, chunk_w, chunk_x_cu_seqlens, chunk_y_cu_seqlens, chunk_x_maxlen, chunk_y_maxlen, \n",
    "                          context_lens=chunk_context_lens, cmp_slot_mapping=chunk_cmp_slot_mapping, cmp_block_tables=cmp_block_tables, block_tables=block_tables)\n",
    "    bos = 0\n",
    "    for j in range(B):\n",
    "        chunk_start = starts[j] + i * chunk_seqlens[j]\n",
    "        chunk_end = min(ends[j], chunk_start + chunk_seqlens[j])\n",
    "        o[chunk_start:chunk_end] = chunk_o[bos:bos+chunk_end-chunk_start]\n",
    "        bos += chunk_end - chunk_start\n",
    "cmp_k = k_cache.flatten(0, 1)[cmp_slot_mapping]\n",
    "compare(o, ref_o)\n",
    "compare(cmp_k, ref_cmp_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
